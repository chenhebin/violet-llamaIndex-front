// ç”Ÿæˆå›ç­”å¼•æ“jsæ–‡ä»¶
import { Settings, VectorStoreIndex } from "llamaindex";
import { getStorageContext } from "../utils/storage.util.js"; // å¯¼å…¥å­˜å‚¨ä¸Šä¸‹æ–‡
import { initializeLLM } from "../services/llm.service.js"; // åˆå§‹åŒ–LLMæ¨¡å‹
import { initializeEmbedding } from "../services/embedding.service.js"; // åˆå§‹åŒ–åµŒå…¥æ¨¡å‹

// åˆå§‹åŒ–å…¨å±€è®¾ç½® - llamaindexåº•å±‚ä¼šè‡ªåŠ¨ä½¿ç”¨Settings.llmå’ŒSettings.embedModelçš„é…ç½®ï¼Œä¸ç”¨æ‰‹åŠ¨æ³¨å…¥
Settings.llm = initializeLLM();
Settings.embedModel = initializeEmbedding();

/**
 * ä»æœ¬åœ°å‘é‡åº“ä¸­æŸ¥è¯¢å¹¶ç”Ÿæˆå›ç­”
 * æ³¨æ„ï¼šè¿™é‡Œçš„queryEngineæ˜¯ä¸€ä¸ªå¼‚æ­¥å‡½æ•°ï¼Œéœ€è¦ç­‰å¾…è¿”å›ç»“æœåæ‰èƒ½ç»§ç»­æ‰§è¡Œåç»­ä»£ç 
 * å¯ä»¥ä½¿ç”¨async/awaitæˆ–è€….then()æ¥å¤„ç†è¿”å›çš„Promiseå¯¹è±¡
 * @param {*} question 
 * @returns 
 */
export async function queryVectors(question) {
    // åŸºäºå·²ç»å®ŒæˆæŒä¹…åŒ–çš„ç›®å½•ï¼Œåˆ›å»ºå‘é‡å­˜å‚¨ç´¢å¼•-ã€storageContextFromDefaultsæ–¹æ³•ã€‘
    const storageContext = await getStorageContext();
    // åŸºäºå‘é‡å­˜å‚¨ç´¢å¼•ï¼Œåˆå§‹åŒ–åˆ›å»ºæŸ¥è¯¢å¼•æ“
    const index = await VectorStoreIndex.init({ storageContext });
    // åˆ›å»ºæŸ¥è¯¢å¼•æ“
    const queryEngine = index.asQueryEngine();

    console.log("ğŸ§  å›ç­”ç”Ÿæˆä¸­...");
    // å‘èµ·æŸ¥è¯¢ï¼Œå¯ä»¥ä½¿ç”¨æµå¼å¤„ç†ã€stram: trueã€‘ï¼Œä½†æ˜¯éœ€è¦é¢å¤–å¤„ç†æµå¼æ•°æ®
    const response = await queryEngine.query({ query: question });
    console.log("ğŸ§  å›ç­”ï¼š", response.toString());
    return response;
}
